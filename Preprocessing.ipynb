{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing untuk Analisis Sentimen\n",
    "\n",
    "Notebook ini melakukan text preprocessing untuk analisis sentimen Twitter tentang sertifikasi halal.\n",
    "\n",
    "**Tahapan Preprocessing:**\n",
    "1. Case Folding (lowercase)\n",
    "2. Cleaning (hapus simbol & angka)\n",
    "3. Tokenizing\n",
    "4. Stopword Removal\n",
    "\n",
    "**PENTING:** Stemming TIDAK digunakan karena merusak kata-kata penting seperti 'sertifikasi' → 'rtifikas'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data hasil cleaning\n",
    "df = pd.read_csv('data/hasil_cleaning.csv')\n",
    "print(f'Jumlah data awal: {len(df)}')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Stopwords\n",
    "\n",
    "Daftar stopword Bahasa Indonesia yang komprehensif."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords Bahasa Indonesia - komprehensif\n",
    "STOPWORDS = set([\n",
    "    # Kata ganti\n",
    "    'saya', 'aku', 'kamu', 'anda', 'dia', 'ia', 'kami', 'kita', 'mereka',\n",
    "    'beliau', 'engkau', 'kalian', 'gue', 'gw', 'lo', 'lu',\n",
    "    # Kata tunjuk\n",
    "    'ini', 'itu', 'sini', 'situ', 'sana',\n",
    "    # Kata depan\n",
    "    'di', 'ke', 'dari', 'pada', 'dalam', 'dengan', 'untuk', 'oleh',\n",
    "    'tentang', 'terhadap', 'kepada', 'bagi', 'antara', 'tanpa',\n",
    "    # Kata hubung\n",
    "    'dan', 'atau', 'tetapi', 'namun', 'melainkan', 'sedangkan', 'serta',\n",
    "    'karena', 'sebab', 'jika', 'kalau', 'bila', 'apabila', 'maka',\n",
    "    'sehingga', 'agar', 'supaya', 'meskipun', 'walaupun', 'walau',\n",
    "    'sebelum', 'sesudah', 'setelah', 'ketika', 'saat', 'sambil', 'seraya',\n",
    "    'bahwa', 'yang', 'yaitu', 'yakni',\n",
    "    # Kata keterangan\n",
    "    'sangat', 'amat', 'sekali', 'paling', 'lebih', 'kurang', 'agak',\n",
    "    'cukup', 'hampir', 'hanya', 'saja', 'pun', 'juga', 'lagi', 'masih',\n",
    "    'sudah', 'telah', 'belum', 'akan', 'sedang', 'baru', 'selalu',\n",
    "    'sering', 'jarang', 'kadang', 'pernah', 'tidak', 'bukan', 'jangan',\n",
    "    'tak', 'tiada',\n",
    "    # Kata bantu\n",
    "    'adalah', 'ialah', 'merupakan', 'dapat', 'bisa',\n",
    "    'mampu', 'harus', 'perlu', 'mau', 'ingin', 'hendak',\n",
    "    # Kata bilangan\n",
    "    'satu', 'dua', 'tiga', 'empat', 'lima', 'enam', 'tujuh', 'delapan',\n",
    "    'sembilan', 'sepuluh', 'banyak', 'sedikit', 'semua', 'seluruh',\n",
    "    'setiap', 'tiap', 'para', 'beberapa', 'sebagian',\n",
    "    # Kata tanya\n",
    "    'apa', 'siapa', 'mana', 'kapan', 'dimana', 'kemana', 'darimana',\n",
    "    'bagaimana', 'mengapa', 'kenapa', 'berapa',\n",
    "    # Kata lainnya\n",
    "    'hal', 'cara', 'sesuatu', 'begitu', 'demikian', 'seperti', 'sebagai',\n",
    "    'secara', 'melalui', 'hingga', 'sampai', 'sejak', 'selama', 'sepanjang',\n",
    "    'tersebut', 'tadi', 'kemarin', 'besok', 'nanti', 'kini', 'sekarang',\n",
    "    'dulu', 'dahulu', 'lalu', 'kemudian', 'lantas',\n",
    "    # Partikel\n",
    "    'nya', 'mu', 'ku', 'kah', 'lah', 'pun',\n",
    "    # Kata gaul/slang Twitter\n",
    "    'dong', 'deh', 'sih', 'nih', 'nah', 'yah', 'ya', 'oh', 'ah',\n",
    "    'eh', 'kok', 'kan', 'tuh', 'gitu', 'gini', 'aja', 'doang',\n",
    "    'banget', 'bgt', 'yg', 'dgn', 'utk', 'krn', 'jd', 'jdi', 'tp', 'tpi',\n",
    "    'sm', 'dr', 'pd', 'dl', 'bs', 'ga', 'gak', 'gk', 'ngga', 'nggak',\n",
    "    'udah', 'udh', 'sdh', 'blm', 'lg', 'jg', 'aj',\n",
    "    'klo', 'kalo', 'emang', 'emg', 'bener', 'bnr', 'gmn', 'gmna'\n",
    "])\n",
    "\n",
    "print(f'Jumlah stopword: {len(STOPWORDS)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Fungsi Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def case_folding(text):\n",
    "    \"\"\"Mengubah semua huruf menjadi lowercase.\"\"\"\n",
    "    if pd.isna(text) or text == '':\n",
    "        return ''\n",
    "    return str(text).lower()\n",
    "\n",
    "\n",
    "def cleaning(text):\n",
    "    \"\"\"Menghapus simbol, angka, dan karakter non-huruf.\"\"\"\n",
    "    if not text:\n",
    "        return ''\n",
    "    # Hapus semua karakter kecuali huruf dan spasi\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "    # Hapus spasi berlebih\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"Memecah teks menjadi list kata (token).\"\"\"\n",
    "    if not text:\n",
    "        return []\n",
    "    tokens = text.split()\n",
    "    # Filter kata yang terlalu pendek (1-2 karakter)\n",
    "    tokens = [t for t in tokens if len(t) > 2]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    \"\"\"Menghapus stopword bahasa Indonesia dari list token.\"\"\"\n",
    "    if not tokens:\n",
    "        return []\n",
    "    return [t for t in tokens if t not in STOPWORDS]\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    \"\"\"\n",
    "    Fungsi utama untuk preprocessing teks.\n",
    "    Tahapan: Case folding -> Cleaning -> Tokenizing -> Stopword removal\n",
    "    TANPA STEMMING untuk menjaga kata tetap utuh.\n",
    "    \"\"\"\n",
    "    if pd.isna(text) or text == '':\n",
    "        return ''\n",
    "    \n",
    "    # 1. Case folding\n",
    "    text = case_folding(text)\n",
    "    \n",
    "    # 2. Cleaning (hapus simbol & angka)\n",
    "    text = cleaning(text)\n",
    "    \n",
    "    # 3. Tokenizing\n",
    "    tokens = tokenize(text)\n",
    "    \n",
    "    # 4. Stopword removal\n",
    "    tokens = remove_stopwords(tokens)\n",
    "    \n",
    "    # Join kembali menjadi string\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test Fungsi Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test preprocessing - kata harus tetap utuh!\n",
    "test_cases = [\n",
    "    \"Sertifikasi halal sangat penting untuk masyarakat Indonesia!!!\",\n",
    "    \"Produk ini sudah memiliki sertifikasi halal dari MUI @user123\",\n",
    "    \"UMKM wajib tahu batas sertifikasi halal kian dekat 2026\"\n",
    "]\n",
    "\n",
    "print(\"Test Preprocessing (TANPA STEMMING - kata tetap utuh):\")\n",
    "print(\"=\"*70)\n",
    "for text in test_cases:\n",
    "    result = preprocess(text)\n",
    "    print(f\"\\nBefore: {text}\")\n",
    "    print(f\"After:  {result}\")\n",
    "\n",
    "# Verifikasi kata penting tetap utuh\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"VERIFIKASI: Kata 'sertifikasi' harus tetap 'sertifikasi', BUKAN 'rtifikas'\")\n",
    "test = preprocess(\"sertifikasi halal\")\n",
    "print(f\"Input: 'sertifikasi halal' -> Output: '{test}'\")\n",
    "assert 'sertifikasi' in test, \"ERROR: Kata sertifikasi rusak!\"\n",
    "print(\"✓ Kata 'sertifikasi' tetap utuh!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Apply Preprocessing ke Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing\n",
    "print(\"Memulai preprocessing...\")\n",
    "print(f\"Jumlah data awal: {len(df)}\")\n",
    "\n",
    "df['preprocessed_text'] = df['cleaned_text'].apply(preprocess)\n",
    "\n",
    "print(\"Preprocessing selesai!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview hasil\n",
    "print(\"Contoh hasil preprocessing:\")\n",
    "print(\"=\"*70)\n",
    "for i in range(min(5, len(df))):\n",
    "    print(f\"\\n--- Data {i+1} ---\")\n",
    "    cleaned = str(df['cleaned_text'].iloc[i])\n",
    "    preprocessed = str(df['preprocessed_text'].iloc[i])\n",
    "    print(f\"Cleaned:      {cleaned[:80]}...\" if len(cleaned) > 80 else f\"Cleaned:      {cleaned}\")\n",
    "    print(f\"Preprocessed: {preprocessed[:80]}...\" if len(preprocessed) > 80 else f\"Preprocessed: {preprocessed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data kosong\n",
    "print(f\"Jumlah data sebelum filter: {len(df)}\")\n",
    "df = df[df['preprocessed_text'].str.len() > 0].reset_index(drop=True)\n",
    "print(f\"Jumlah data setelah filter: {len(df)}\")\n",
    "\n",
    "# Info dataset\n",
    "print(f\"\\nKolom: {df.columns.tolist()}\")\n",
    "df[['cleaned_text', 'preprocessed_text']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Simpan Hasil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simpan hasil preprocessing\n",
    "df.to_csv('data/hasil_preprocessing.csv', index=False)\n",
    "print(\"Data berhasil disimpan ke data/hasil_preprocessing.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Statistik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 30 kata paling sering muncul\n",
    "all_words = ' '.join(df['preprocessed_text']).split()\n",
    "word_freq = Counter(all_words)\n",
    "\n",
    "print(\"Top 30 kata paling sering muncul:\")\n",
    "print(\"=\"*40)\n",
    "for i, (word, count) in enumerate(word_freq.most_common(30), 1):\n",
    "    print(f\"{i:2}. {word}: {count}\")\n",
    "\n",
    "# Verifikasi kata penting\n",
    "important_words = ['sertifikasi', 'halal', 'makanan', 'produk', 'muslim', 'indonesia', 'haram']\n",
    "print(\"\\nVerifikasi kata penting (harus tetap utuh):\")\n",
    "print(\"=\"*40)\n",
    "for word in important_words:\n",
    "    count = word_freq.get(word, 0)\n",
    "    print(f\"{word}: {count} kemunculan\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ringkasan\n",
    "\n",
    "Preprocessing selesai dengan tahapan:\n",
    "1. **Case Folding** - Mengubah semua huruf menjadi lowercase\n",
    "2. **Cleaning** - Menghapus simbol, angka, dan karakter non-huruf\n",
    "3. **Tokenizing** - Memecah teks menjadi kata-kata (filter kata < 3 karakter)\n",
    "4. **Stopword Removal** - Menghapus kata-kata tidak penting\n",
    "\n",
    "**TANPA STEMMING** - Kata seperti 'sertifikasi', 'halal' tetap utuh."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
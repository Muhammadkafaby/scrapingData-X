{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ***ANALISIS SENTIMEN DATA SERTIFIKASI HALAL***\n",
                "# ***DENGAN PENERAPAN K-MEANS CLUSTERING***"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ***LOAD DATA***"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Load data sertifikasi halal\n",
                "data = pd.read_csv('data/dataSertifikasiHalal.csv')\n",
                "data.info()\n",
                "data.head(5)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ubah kolom ke datetime (aman jika masih string)\n",
                "data['created_at'] = pd.to_datetime(data['created_at'])\n",
                "\n",
                "# pisahkan tanggal dan waktu\n",
                "data['tanggal'] = data['created_at'].dt.date\n",
                "data['waktu']   = data['created_at'].dt.time\n",
                "\n",
                "data.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Buat dataframe dengan kolom yang diperlukan\n",
                "df = pd.DataFrame(data[['tanggal', 'waktu', 'full_text', 'favorite_count', 'retweet_count', 'username']])\n",
                "df.columns = ['tanggal', 'waktu', 'content', 'likes', 'retweets', 'username']\n",
                "df.head(5)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ***PREPROCESSING DATA***"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**PROSES HAPUS DATA DUPLIKAT**\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df.info()\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# jumlah data duplikat\n",
                "data_duplikat = df[df.duplicated(subset='content', keep=False)]\n",
                "print(f'Jumlah data duplikat: {len(data_duplikat)}')\n",
                "data_duplikat.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# hapus data duplikat\n",
                "df = df.drop_duplicates(subset='content', keep='first')\n",
                "print(f'Jumlah data setelah hapus duplikat: {len(df)}')\n",
                "df.info()\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# hapus data null\n",
                "df = df.dropna(subset=['content'])\n",
                "print(f'Jumlah data setelah hapus null: {len(df)}')\n",
                "df.info()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**PROSES CASE FOLDING (lowercase)**\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Case folding - mengubah semua huruf menjadi lowercase\n",
                "df['content_lower'] = df['content'].str.lower()\n",
                "df[['content', 'content_lower']].head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**PROSES CLEANING (Hapus URL, mention, hashtag, angka, tanda baca)**\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import re\n",
                "import string\n",
                "\n",
                "def clean_text(text):\n",
                "    if pd.isna(text):\n",
                "        return \"\"\n",
                "    \n",
                "    # Hapus URL\n",
                "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
                "    \n",
                "    # Hapus mentions (@username)\n",
                "    text = re.sub(r'@\\w+', '', text)\n",
                "    \n",
                "    # Hapus hashtags (#hashtag)\n",
                "    text = re.sub(r'#\\w+', '', text)\n",
                "    \n",
                "    # Hapus angka\n",
                "    text = re.sub(r'\\d+', '', text)\n",
                "    \n",
                "    # Hapus tanda baca\n",
                "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
                "    \n",
                "    # Hapus karakter spesial\n",
                "    text = re.sub(r'[^\\w\\s]', '', text)\n",
                "    \n",
                "    # Hapus newline dan tab\n",
                "    text = re.sub(r'[\\n\\t\\r]', ' ', text)\n",
                "    \n",
                "    # Hapus spasi berlebih\n",
                "    text = re.sub(r'\\s+', ' ', text).strip()\n",
                "    \n",
                "    return text\n",
                "\n",
                "df['content_clean'] = df['content_lower'].apply(clean_text)\n",
                "df[['content', 'content_clean']].head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**PROSES TOKENIZING**\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import nltk\n",
                "try:\n",
                "    nltk.data.find('tokenizers/punkt')\n",
                "except LookupError:\n",
                "    nltk.download('punkt')\n",
                "    nltk.download('stopwords')\n",
                "    nltk.download('punkt_tab')\n",
                "\n",
                "from nltk.tokenize import word_tokenize\n",
                "\n",
                "def tokenize_text(text):\n",
                "    if pd.isna(text) or text == \"\":\n",
                "        return []\n",
                "    return word_tokenize(text)\n",
                "\n",
                "df['content_tokens'] = df['content_clean'].apply(tokenize_text)\n",
                "df[['content_clean', 'content_tokens']].head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**PROSES STOPWORD REMOVAL**\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
                "from nltk.corpus import stopwords\n",
                "\n",
                "# Gabungkan stopwords dari NLTK Indonesia dan Sastrawi\n",
                "stop_words_nltk = set(stopwords.words('indonesian'))\n",
                "stop_factory = StopWordRemoverFactory()\n",
                "stop_words_sastrawi = set(stop_factory.get_stop_words())\n",
                "\n",
                "# Custom stopwords untuk konteks sertifikasi halal\n",
                "custom_stopwords = {\n",
                "    'yang', 'dan', 'di', 'ke', 'dari', 'ini', 'itu', 'untuk', 'dengan',\n",
                "    'adalah', 'pada', 'juga', 'tidak', 'ada', 'akan', 'bisa', 'sudah',\n",
                "    'ya', 'ga', 'gak', 'ngga', 'nggak', 'aja', 'saja', 'kan', 'dong',\n",
                "    'sih', 'nih', 'tuh', 'deh', 'yuk', 'yg', 'dgn', 'utk', 'dlm', 'krn',\n",
                "    'rt', 'amp', 'via'\n",
                "}\n",
                "\n",
                "all_stopwords = stop_words_nltk.union(stop_words_sastrawi).union(custom_stopwords)\n",
                "\n",
                "def remove_stopwords(tokens):\n",
                "    return [word for word in tokens if word not in all_stopwords and len(word) > 2]\n",
                "\n",
                "df['content_no_stopword'] = df['content_tokens'].apply(remove_stopwords)\n",
                "df[['content_tokens', 'content_no_stopword']].head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**PROSES STEMMING**\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
                "\n",
                "print(\"Memulai proses stemming (ini membutuhkan waktu)...\")\n",
                "\n",
                "stem_factory = StemmerFactory()\n",
                "stemmer = stem_factory.create_stemmer()\n",
                "\n",
                "def stem_tokens(tokens):\n",
                "    return [stemmer.stem(word) for word in tokens]\n",
                "\n",
                "df['content_stemmed'] = df['content_no_stopword'].apply(stem_tokens)\n",
                "df[['content_no_stopword', 'content_stemmed']].head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Join tokens menjadi string\n",
                "df['content_final'] = df['content_stemmed'].apply(lambda x: ' '.join(x))\n",
                "\n",
                "# Hapus baris dengan content_final kosong\n",
                "df = df[df['content_final'].str.len() > 0]\n",
                "print(f\"Data setelah preprocessing: {len(df)} baris\")\n",
                "\n",
                "df[['content', 'content_final']].head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ***PELABELAN DATA SENTIMEN***\n",
                "\n",
                "Karena data tweet tidak memiliki rating/score seperti data Play Store, kita akan melakukan pelabelan sentimen menggunakan:\n",
                "1. Lexicon-based labeling (kata kunci positif/negatif)\n",
                "2. Validasi dengan K-Means Clustering"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Kata-kata positif dan negatif untuk labeling awal\n",
                "kata_positif = [\n",
                "    'bagus', 'baik', 'senang', 'puas', 'mantap', 'keren', 'hebat', \n",
                "    'excellent', 'good', 'great', 'aman', 'terjamin', 'percaya', \n",
                "    'halal', 'berkualitas', 'resmi', 'sertifikat', 'jelas', 'setuju',\n",
                "    'dukung', 'support', 'wajib', 'penting', 'benar', 'betul'\n",
                "]\n",
                "\n",
                "kata_negatif = [\n",
                "    'buruk', 'jelek', 'kecewa', 'marah', 'kesal', 'tidak', 'gagal',\n",
                "    'ribet', 'susah', 'lama', 'mahal', 'curang', 'haram', 'palsu',\n",
                "    'bohong', 'tipu', 'masalah', 'komplain', 'protes', 'tolak',\n",
                "    'salah', 'kontroversi', 'konstitusi', 'diskriminasi', 'babi'\n",
                "]\n",
                "\n",
                "def label_sentimen(text):\n",
                "    if pd.isna(text):\n",
                "        return 'Netral'\n",
                "    \n",
                "    text_lower = text.lower()\n",
                "    pos_count = sum(1 for kata in kata_positif if kata in text_lower)\n",
                "    neg_count = sum(1 for kata in kata_negatif if kata in text_lower)\n",
                "    \n",
                "    if pos_count > neg_count:\n",
                "        return 'Positif'\n",
                "    elif neg_count > pos_count:\n",
                "        return 'Negatif'\n",
                "    else:\n",
                "        return 'Netral'\n",
                "\n",
                "df['sentimen_lexicon'] = df['content_final'].apply(label_sentimen)\n",
                "\n",
                "print(\"Distribusi Sentimen (Lexicon-based):\")\n",
                "print(df['sentimen_lexicon'].value_counts())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualisasi distribusi sentimen lexicon\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "plt.figure(figsize=(10, 6))\n",
                "colors = ['#6bcb77', '#ffd93d', '#ff6b6b']\n",
                "sentimen_counts = df['sentimen_lexicon'].value_counts()\n",
                "\n",
                "plt.bar(sentimen_counts.index, sentimen_counts.values, color=colors, edgecolor='black')\n",
                "plt.xlabel('Sentimen', fontsize=12)\n",
                "plt.ylabel('Jumlah', fontsize=12)\n",
                "plt.title('Distribusi Sentimen (Lexicon-based)\\nData Sertifikasi Halal', fontsize=14, fontweight='bold')\n",
                "\n",
                "for i, (idx, v) in enumerate(zip(sentimen_counts.index, sentimen_counts.values)):\n",
                "    plt.text(i, v + 10, str(v), ha='center', fontsize=11, fontweight='bold')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('output/distribusi_sentimen_lexicon.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ***TF-IDF VECTORIZATION***"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.feature_extraction.text import TfidfVectorizer\n",
                "\n",
                "# TF-IDF Vectorizer\n",
                "tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n",
                "tfidf_matrix = tfidf.fit_transform(df['content_final'])\n",
                "\n",
                "print(f\"TF-IDF Matrix shape: {tfidf_matrix.shape}\")\n",
                "print(f\"Vocabulary size: {len(tfidf.vocabulary_)}\")\n",
                "\n",
                "# Tampilkan top features\n",
                "feature_names = tfidf.get_feature_names_out()\n",
                "print(f\"\\nTop 20 features: {feature_names[:20].tolist()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ***PENERAPAN K-MEANS CLUSTERING***"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**MENENTUKAN JUMLAH CLUSTER OPTIMAL (Elbow & Silhouette)**\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.cluster import KMeans\n",
                "from sklearn.metrics import silhouette_score\n",
                "\n",
                "# Elbow Method\n",
                "inertias = []\n",
                "silhouette_scores = []\n",
                "K_range = range(2, 11)\n",
                "\n",
                "print(\"Menghitung inertia dan silhouette score untuk K=2 hingga K=10...\")\n",
                "for k in K_range:\n",
                "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
                "    kmeans.fit(tfidf_matrix)\n",
                "    inertias.append(kmeans.inertia_)\n",
                "    silhouette_scores.append(silhouette_score(tfidf_matrix, kmeans.labels_))\n",
                "    print(f\"K={k}: Inertia={kmeans.inertia_:.2f}, Silhouette={silhouette_scores[-1]:.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot Elbow dan Silhouette\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "# Elbow Plot\n",
                "axes[0].plot(K_range, inertias, 'bo-', linewidth=2, markersize=8)\n",
                "axes[0].set_xlabel('Jumlah Cluster (K)', fontsize=12)\n",
                "axes[0].set_ylabel('Inertia (SSE)', fontsize=12)\n",
                "axes[0].set_title('Elbow Method', fontsize=14)\n",
                "axes[0].grid(True, alpha=0.3)\n",
                "\n",
                "# Silhouette Plot\n",
                "axes[1].plot(K_range, silhouette_scores, 'go-', linewidth=2, markersize=8)\n",
                "axes[1].set_xlabel('Jumlah Cluster (K)', fontsize=12)\n",
                "axes[1].set_ylabel('Silhouette Score', fontsize=12)\n",
                "axes[1].set_title('Silhouette Score', fontsize=14)\n",
                "axes[1].grid(True, alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('output/elbow_silhouette_kmeans.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "# Pilih K optimal berdasarkan silhouette score tertinggi\n",
                "optimal_k = list(K_range)[np.argmax(silhouette_scores)]\n",
                "print(f\"\\nK optimal berdasarkan Silhouette Score: {optimal_k}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**CLUSTERING DENGAN K=3 (Negatif, Netral, Positif)**\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Gunakan K=3 untuk sentimen (Negatif, Netral, Positif)\n",
                "K = 3\n",
                "print(f\"Menggunakan K={K} untuk clustering sentimen (Negatif, Netral, Positif)\")\n",
                "\n",
                "kmeans_final = KMeans(n_clusters=K, random_state=42, n_init=10)\n",
                "df['cluster'] = kmeans_final.fit_predict(tfidf_matrix)\n",
                "\n",
                "print(f\"\\nDistribusi Cluster:\")\n",
                "print(df['cluster'].value_counts().sort_index())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Labeling sentimen berdasarkan cluster\n",
                "def hitung_skor_cluster(cluster_id):\n",
                "    texts = df[df['cluster'] == cluster_id]['content_final']\n",
                "    all_text = ' '.join(texts)\n",
                "    \n",
                "    positif_count = sum(1 for kata in kata_positif if kata in all_text)\n",
                "    negatif_count = sum(1 for kata in kata_negatif if kata in all_text)\n",
                "    \n",
                "    return positif_count - negatif_count\n",
                "\n",
                "cluster_scores = {i: hitung_skor_cluster(i) for i in range(K)}\n",
                "print(\"Skor sentimen per cluster:\")\n",
                "for cluster, score in cluster_scores.items():\n",
                "    print(f\"  Cluster {cluster}: {score}\")\n",
                "\n",
                "# Urutkan cluster berdasarkan skor\n",
                "sorted_clusters = sorted(cluster_scores.items(), key=lambda x: x[1])\n",
                "label_mapping = {}\n",
                "labels = ['Negatif', 'Netral', 'Positif']\n",
                "\n",
                "for i, (cluster_id, score) in enumerate(sorted_clusters):\n",
                "    label_mapping[cluster_id] = labels[i]\n",
                "\n",
                "print(f\"\\nLabel Mapping: {label_mapping}\")\n",
                "\n",
                "# Apply label\n",
                "df['sentimen_kmeans'] = df['cluster'].map(label_mapping)\n",
                "\n",
                "print(\"\\nDistribusi Sentimen (K-Means):\")\n",
                "print(df['sentimen_kmeans'].value_counts())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualisasi distribusi sentimen K-Means\n",
                "plt.figure(figsize=(10, 8))\n",
                "colors = ['#ff6b6b', '#ffd93d', '#6bcb77']\n",
                "sentimen_counts = df['sentimen_kmeans'].value_counts()\n",
                "\n",
                "plt.pie(sentimen_counts.values, \n",
                "        labels=sentimen_counts.index, \n",
                "        autopct='%1.1f%%',\n",
                "        colors=colors,\n",
                "        explode=[0.02] * len(sentimen_counts),\n",
                "        shadow=True,\n",
                "        startangle=90)\n",
                "\n",
                "plt.title('Distribusi Sentimen Data Sertifikasi Halal\\n(K-Means Clustering)', fontsize=14, fontweight='bold')\n",
                "plt.tight_layout()\n",
                "plt.savefig('output/distribusi_sentimen_kmeans.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ***WORDCLOUD***"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from wordcloud import WordCloud\n",
                "\n",
                "# Fix for numpy 2.0 compatibility\n",
                "np.asarray_orig = np.asarray\n",
                "def asarray_fix(*args, **kwargs):\n",
                "    kwargs.pop('copy', None)\n",
                "    return np.asarray_orig(*args, **kwargs)\n",
                "np.asarray = asarray_fix\n",
                "\n",
                "def generate_wordcloud(text, title, color, filename):\n",
                "    \"\"\"Generate word cloud untuk teks tertentu\"\"\"\n",
                "    wordcloud = WordCloud(\n",
                "        width=800, \n",
                "        height=400,\n",
                "        background_color='white',\n",
                "        colormap=color,\n",
                "        max_words=100,\n",
                "        min_font_size=10,\n",
                "        max_font_size=100,\n",
                "        random_state=42\n",
                "    ).generate(text)\n",
                "    \n",
                "    plt.figure(figsize=(12, 6))\n",
                "    plt.imshow(wordcloud, interpolation='bilinear')\n",
                "    plt.axis('off')\n",
                "    plt.title(title, fontsize=16, fontweight='bold', pad=20)\n",
                "    plt.tight_layout()\n",
                "    plt.savefig(f'output/{filename}', dpi=300, bbox_inches='tight')\n",
                "    plt.show()\n",
                "    print(f\"Word cloud disimpan: output/{filename}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Word Cloud untuk semua data\n",
                "all_text = ' '.join(df['content_final'].dropna().astype(str))\n",
                "generate_wordcloud(all_text, 'Word Cloud - Semua Data Sertifikasi Halal', 'viridis', 'wordcloud_all.png')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Word Cloud per sentimen\n",
                "sentimen_colors = {\n",
                "    'Negatif': 'Reds',\n",
                "    'Netral': 'Greys',\n",
                "    'Positif': 'Greens'\n",
                "}\n",
                "\n",
                "for sentimen, color in sentimen_colors.items():\n",
                "    text = ' '.join(df[df['sentimen_kmeans'] == sentimen]['content_final'].dropna().astype(str))\n",
                "    if text.strip():\n",
                "        generate_wordcloud(\n",
                "            text, \n",
                "            f'Word Cloud - Sentimen {sentimen}', \n",
                "            color, \n",
                "            f'wordcloud_{sentimen.lower()}.png'\n",
                "        )"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ***FREKUENSI KATA***"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from collections import Counter\n",
                "\n",
                "def get_top_words(df, sentimen, n=20):\n",
                "    \"\"\"Get top n words untuk sentimen tertentu\"\"\"\n",
                "    text = ' '.join(df[df['sentimen_kmeans'] == sentimen]['content_final'].dropna().astype(str))\n",
                "    words = text.split()\n",
                "    word_counts = Counter(words)\n",
                "    return word_counts.most_common(n)\n",
                "\n",
                "for sentimen in ['Positif', 'Netral', 'Negatif']:\n",
                "    print(f\"\\n{'='*50}\")\n",
                "    print(f\"Top 15 Kata - Sentimen {sentimen}:\")\n",
                "    print(f\"{'='*50}\")\n",
                "    top_words = get_top_words(df, sentimen, 15)\n",
                "    for i, (word, count) in enumerate(top_words, 1):\n",
                "        print(f\"  {i:2}. {word:20} : {count}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualisasi Top 10 Kata per Sentimen\n",
                "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
                "\n",
                "colors_bar = {'Positif': '#6bcb77', 'Netral': '#ffd93d', 'Negatif': '#ff6b6b'}\n",
                "\n",
                "for idx, sentimen in enumerate(['Positif', 'Netral', 'Negatif']):\n",
                "    top_words = get_top_words(df, sentimen, 10)\n",
                "    words = [w[0] for w in top_words]\n",
                "    counts = [w[1] for w in top_words]\n",
                "    \n",
                "    axes[idx].barh(words[::-1], counts[::-1], color=colors_bar[sentimen], edgecolor='black')\n",
                "    axes[idx].set_xlabel('Frekuensi', fontsize=11)\n",
                "    axes[idx].set_title(f'Top 10 Kata - {sentimen}', fontsize=12, fontweight='bold')\n",
                "    \n",
                "plt.tight_layout()\n",
                "plt.savefig('output/frekuensi_kata_per_sentimen.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ***SIMPAN HASIL***"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Simpan hasil ke CSV\n",
                "output_df = df[['tanggal', 'waktu', 'content', 'username', 'content_final', 'cluster', 'sentimen_lexicon', 'sentimen_kmeans']]\n",
                "output_df.to_csv('output/hasil_analisis_sentimen_sertifikasi_halal.csv', index=False, encoding='utf-8-sig')\n",
                "print(\"Hasil disimpan ke: output/hasil_analisis_sentimen_sertifikasi_halal.csv\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Ringkasan\n",
                "print(\"=\"*60)\n",
                "print(\"RINGKASAN HASIL ANALISIS\")\n",
                "print(\"=\"*60)\n",
                "print(f\"Total Data Awal: {len(data)}\")\n",
                "print(f\"Total Data Setelah Preprocessing: {len(df)}\")\n",
                "print(f\"Jumlah Cluster: {K}\")\n",
                "print(f\"Silhouette Score: {silhouette_score(tfidf_matrix, kmeans_final.labels_):.4f}\")\n",
                "print(f\"\\nDistribusi Sentimen (K-Means):\")\n",
                "print(df['sentimen_kmeans'].value_counts())\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"ANALISIS SELESAI!\")\n",
                "print(\"=\"*60)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}